{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3fe965c",
   "metadata": {},
   "source": [
    "# Pertemuan 09 Data Science 10\n",
    "### Sumber : https://youtu.be/U30sF4m0bd0\n",
    "### Mengenal Text Processing : Bag of Wrods & Stop WOrd Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493ee97a",
   "metadata": {},
   "source": [
    "#### Bag of Words model sebagai representasi text\n",
    "Bag of Words menyederhanakan representasi text sebagai sekumpulan kata serta mengabaikan grammar dan posisi tiap kata pada kalimat. Text akan dikonversi menjadi lowercase dan tanda baca akan diabaikan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b10877",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dd48175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Linux has been around since the mid-1990s.',\n",
       " 'Linux distributions include the linux kernel.',\n",
       " 'Linux is one fo the most prominent open-source software.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['Linux has been around since the mid-1990s.',\n",
    "         'Linux distributions include the linux kernel.',\n",
    "         'Linux is one fo the most prominent open-source software.'\n",
    "         ]\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a08d07e",
   "metadata": {},
   "source": [
    "dataset yang berupa text pendek disebut juga sebagai corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95b9340",
   "metadata": {},
   "source": [
    "#### Bag of Words model dengan `CountVectorizer`\n",
    "bag of Words model dapat diterapkan dnegan memanfaatkan `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11c4ac9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "        [0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1]],\n",
       "       dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorized_X = vectorizer.fit_transform(corpus).todense()\n",
    "vectorized_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef9b25e",
   "metadata": {},
   "source": [
    "untuk melakukan CountVectorizer :\n",
    "- melakukan import modul `from sklearn.feature_extraction.text import CountVectorizer`.\n",
    "- membentuk object dari kelas `CountVectorizer()`\n",
    "- lalu menggunakan `fit_transform()` untuk dataset corpus. \n",
    "- lalu menggunakan `todense()` akan melakukan konfersi dari object `CountVectorizer` menjadi object dua dimensi.\n",
    "\n",
    "setiap bentuk matrix adalah bentuk representasi adalah representasi dari sebuah kalimat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3548b944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1990s',\n",
       " 'around',\n",
       " 'been',\n",
       " 'distributions',\n",
       " 'fo',\n",
       " 'has',\n",
       " 'include',\n",
       " 'is',\n",
       " 'kernel',\n",
       " 'linux',\n",
       " 'mid',\n",
       " 'most',\n",
       " 'one',\n",
       " 'open',\n",
       " 'prominent',\n",
       " 'since',\n",
       " 'software',\n",
       " 'source',\n",
       " 'the']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632df75e",
   "metadata": {},
   "source": [
    "waktu menggunakan `vectorizer.get_feature_names()` akan mengembalikan kata dari bag, teknik ini namanya back of words, sebuah kata dari keranjang atau bag dan diurutkan secara alphabetical order. semua case sudah menjadi lowercase, dan tidak ada tanda baca, kata kata yang sudah di bentuk ini disebut token. nilai pada array tidak hanya nol dan atu setiap nilai meresentasikan, jumlah kemunculan token/kata tertentu pada kalimat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db27c4f2",
   "metadata": {},
   "source": [
    "#### Euclidean Distance untuk mengukur kedekatan/jarak antar dokumen (vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06ef3ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jarak dokumen 1 dan 2 : [[3.16227766]]\n",
      "Jarak dokumen 1 dan 3 : [[3.74165739]]\n",
      "Jarak dokumen 2 dan 3 : [[3.46410162]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "for i in range(len(vectorized_X)):\n",
    "    for j in range(i, len(vectorized_X)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        jarak = euclidean_distances(vectorized_X[i], vectorized_X[j])\n",
    "        print(f'Jarak dokumen {i+1} dan {j+1} : {jarak}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6958ad1",
   "metadata": {},
   "source": [
    "dengan representasikan back of word dapat dengan mudah mengukur kedekatan atau kemiripan antar dokumen. untuk mengukur kedekatan menggunakan `euclidean_distances`, caranya :\n",
    "- melakukan import `from sklearn.metrics.pairwise import euclidean_distances`\n",
    "- mengukur jarak antar kalimat kalimat pertama akan di ukur dengan kalimat kedua `for i in range(len(vectorized_X)):`\n",
    "- mengukur jarak antar kalimat kalimat kedua akan di ukur dengan kalimat ketiga `for j in range(i, len(vectorized_X)):`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e7ec12",
   "metadata": {},
   "source": [
    "#### Stop Word filtering pada text\n",
    "Stop Word Filtering menyederhanakan repsentasi text dengan mengabaikan beberapa kata seperti determiners (the,a,an), auxiliary verbs (do,be,will), dan prepositions (on,it,at)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2948b677",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a494f776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Linux has been around since the mid-1990s.',\n",
       " 'Linux distributions include the linux kernel.',\n",
       " 'Linux is one fo the most prominent open-source software.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb939a1",
   "metadata": {},
   "source": [
    "#### Stop Word Filtering dengan `CountVectorizer`\n",
    "Stop Word Filtering juga dapat diterapkan dengan memanfatkan `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09b70531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 1, 1, 2, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorized_X = vectorizer.fit_transform(corpus).todense()\n",
    "vectorized_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db20d97f",
   "metadata": {},
   "source": [
    "cara melakukan CountVectorizer stop word filtering :\n",
    "- melakukan import `from sklearn.feature_extraction.text import CountVectorizer`\n",
    "- menyertakan parameter pada `CountVectorizer` yaitu `stop_words='english'`\n",
    "- lalu menggunakan `fit_transform()` untuk `corpus` dan melakukan `todence()` untuk membangun array dua dimensi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6b5af8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1990s',\n",
       " 'distributions',\n",
       " 'fo',\n",
       " 'include',\n",
       " 'kernel',\n",
       " 'linux',\n",
       " 'mid',\n",
       " 'open',\n",
       " 'prominent',\n",
       " 'software',\n",
       " 'source']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80679c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
